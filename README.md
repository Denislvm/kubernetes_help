# Kubernetes draft\workbook

## What elements does it consist of

## 
 0. Container (Docker)
 1. Pod - это объект который состоит из 1 или нескольких контейнеров. 
 2. Deployment - состоит из 1 или нескольких одинаковых Pod'oв. Набор одинаковых Pods, который нужен для дальнейшего AutoScaling и для обновления Container Image, держит минимальное количество работающих Pods.
 3. Service - дает доступ к Pod которые бегут в Deployment. Предоставляет доступ к Deployment через:
 - ClusterIP
 - NodePort
 - LoadBalancer
 - ExternalName 
 4. Nodes - Сервера в которых все и работает 

 5. Cluster - это набор рабочих узлов, которые работают вместе для запуска контейнерных приложений. Кластер Kubernetes является предварительным требованием для развертывания ArcGIS Enterprise on Kubernetes.

### Kubectl
    Kubectl — это инструмент командной строки для управления кластерами Kubernetes. kubectl ищет файл config в директории $HOME/. kube. Вы можете указать другие файлы kubeconfig, установив переменную окружения KUBECONFIG или флаг --kubeconfig



## Job
    Job: сущность для разовых задач

Job (работа, задание) — это yaml-манифест, который создаёт под для выполнения разовой задачи. Если запуск задачи завершается с ошибкой, Job перезапускает поды до успешного выполнения или до истечения таймаутов. Когда задача выполнена, Job считается завершённым и больше никогда в кластере не запускается. Job — это сущность для разовых задач.

## Когда используют Job

При установке и настройке окружения. Например, мы построили CI/CD, который при создании новой ветки автоматически создаёт для неё окружение для тестирования. Появилась ветка — в неё пошли коммиты — CI/CD создал в кластере отдельный namespace и запустил Job — тот, в свою очередь, создал базу данных, налил туда данные, все конфиги сохранил в Secret и ConfigMap. То есть Job подготовил цельное окружение, на котором можно тестировать и отлаживать новую функциональность.

 # Engl documentation 

    Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications.

    Every cluster has at least one worker node.

## Control Plane Components
    The control plane's components make global decisions about the cluster (for example, scheduling), as well as detecting and responding to cluster events (for example, starting up a new pod when a deployment's replicas field is unsatisfied).

## etcd
Состояние мастера хранится в экземпляре etcd. Это обеспечивает надёжное хранение конфигурационных данных и своевременное оповещение прочих компонентов об изменении состояния.

    Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.
    If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for those data.
## Scheduler

Scheduler привязывает незапущенные pod'ы к нодам через вызов /binding API. Scheduler подключаем; планируется поддержка множественных scheduler'ов и пользовательских scheduler'ов.
## kube-scheduler
    Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.

## kube-controller-manager

    Control plane component that runs controller processes.

    Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.

    Some types of these controllers are:
    1) Node controller: Responsible for noticing and responding when nodes go down.
    2) Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.
    3) EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods).
    4) ServiceAccount controller: Create default ServiceAccounts for new namespaces.
## Kube-Proxy

Также на каждой ноде запускается простой proxy-балансировщик. Этот сервис запускается на каждой ноде и настраивается в Kubernetes API. Kube-Proxy может выполнять простейшее перенаправление потоков TCP и UDP (round robin) между набором бэкендов.
---------------------------------------
# Node Components
При взгляде на архитектуру системы мы можем разбить его на сервисы, которые работают на каждой ноде и сервисы уровня управления кластера. На каждой ноде Kubernetes запускаются сервисы, необходимые для управления нодой со стороны мастера и для запуска приложений. Конечно, на каждой ноде запускается Docker. Docker обеспечивает загрузку образов и запуск контейнеров.

    Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.

## kubelet

Kubelet управляет pod'ами их контейнерами, образами, разделами, etc.
    An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.

    The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.
    
## Kubernetes Controller Manager Server

Все остальные функции уровня кластера представлены в Controller Manager. Например, ноды обнаруживаются, управляются и контролируются средствами node controller. Эта сущность в итоге может быть разделена на отдельные компоненты, чтобы сделать их независимо подключаемыми.

ReplicationController — это механизм, основывающийся на pod API. В конечном счете планируется перевести её на общий механизм plug-in, когда он будет реализован.

## Четное количество нод, имеющих право стать мастер-нодой
    Обычно в кластере должно быть нечетное количество узлов с ролью мастер. Если число четное, Elasticsearch оставляет один из них вне конфигурации голосования, чтобы гарантировать, что общее количество нод с ролью мастер нечетно. Это не снижает отказоустойчивость кластера. Фактически, даже немного улучшает его: если кластер страдает от сетевого разделения (например, при пропадания связности двух ЦОДов), который делит его на две половины одинакового размера, тогда одна из половин будет содержать большую часть конфигурации голосования и сможет продолжать работу. Если бы были подсчитаны все голоса от узлов, имеющих право голоса, ни одна из сторон не содержала бы строгое большинство узлов, и поэтому кластер не смог бы штатно работать.

    Например, если в кластере четыре основных узла, имеющих право на голосование, и конфигурация голосования учитывает их, для принятия любого решения на основе кворума потребуются голоса как минимум трех из них. Эта ситуация означает, что кластер может выдержать потерю только одного основного узла. Если бы этот кластер был разделен на две равные половины, ни одна из половин не содержала бы трех основных узлов, и кластер не смог бы выбрать мастер-ноду. Если конфигурация голосования содержит только три из четырех основных узлов, имеющих право голоса, кластер по-прежнему полностью толерантен только к потере одного узла, но решения на основе кворума требуют голосов от двух из трех узлов с правом голоса. В случае равного разделения одна половина будет содержать два из трех узлов голосования, так что половина останется доступной.

## Настройка начальной конфигурации голосования
    Когда новый кластер запускается впервые, он должен выбрать свою первую мастер-ноду. Для этого выбора ему необходимо знать набор узлов, имеющих право голоса. Эта первоначальная конфигурация голосования известна как конфигурация начальной загрузки и устанавливается в процессе начальной загрузки кластера.

    Важно, чтобы конфигурация начальной загрузки точно определяла, какие узлы должны голосовать на первых выборах. Недостаточно настроить каждый узел с расчетом на то, сколько узлов должно быть в кластере. Конфигурация начальной загрузки должна поступать извне кластера: у кластера нет безопасного способа правильно определить конфигурацию начальной загрузки самостоятельно.

    Если конфигурация начальной загрузки установлена ​​неправильно, при запуске нового кластера существует риск того, что вы случайно сформируете два отдельных кластера вместо одного. Эта ситуация может привести к потере данных: вы можете начать использовать оба кластера до того, как заметите, что что-то пошло не так, и их невозможно позже объединить.

    Чтобы проиллюстрировать проблему с настройкой каждого узла на ожидание определенного размера кластера, представьте, что запускаете трехузловой кластер, в котором каждый узел знает, что он будет частью трехузлового кластера. Большинство из трех узлов равно двум, поэтому обычно первые два узла, которые обнаруживают друг друга, образуют кластер, а третий узел присоединяется к ним через некоторое время. Теперь представьте, что вместо трех были ошибочно запущены четыре узла. В этом случае узлов достаточно, чтобы образовать два отдельных кластера. Конечно, если каждый узел запускается вручную, маловероятно, что будет запущено слишком много узлов. Однако, если вы используете автоматизированный оркестратор, можно попасть в такую ​​ситуацию, особенно если оркестратор не устойчив к сбоям, таким как сетевое разделение.

    Первоначальный кворум требуется только при первом запуске всего кластера. Новые узлы, присоединяющиеся к установленному кластеру, могут безопасно получать всю необходимую информацию от выбранного мастера. Узлы, которые ранее были частью кластера, будут хранить на диске всю информацию, которая требуется при их перезапуске.
